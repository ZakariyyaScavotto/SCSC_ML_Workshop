{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><b>MACHINE LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> In this notebook, we will be looking at trying to predict the genre of a song based on a variety of features\n",
    "\n",
    "<h3> Run cells using shift + enter or the run button at the top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You may need to install sklearn and xgboost if you haven't already from last week's workshop\\nONLY UNCOMMENT AND RUN IT IF NEEDED, OTHERWISE YOU CAN SKIP THIS CELL\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"You may need to install sklearn and xgboost if you haven't already from last week's workshop\n",
    "ONLY UNCOMMENT AND RUN IT IF NEEDED, OTHERWISE YOU CAN SKIP THIS CELL\"\"\"\n",
    "# %pip install scikit-learn\n",
    "# %pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><b>Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Track</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Year</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Time_Signature</th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Key</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Instrumentalness</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hey Jack Kerouac</td>\n",
       "      <td>10,000 Maniacs</td>\n",
       "      <td>1987</td>\n",
       "      <td>206413</td>\n",
       "      <td>4</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.511</td>\n",
       "      <td>6</td>\n",
       "      <td>-15.894</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>0.03840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.604</td>\n",
       "      <td>132.015</td>\n",
       "      <td>40</td>\n",
       "      <td>Alt. Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Like the Weather</td>\n",
       "      <td>10,000 Maniacs</td>\n",
       "      <td>1987</td>\n",
       "      <td>236653</td>\n",
       "      <td>4</td>\n",
       "      <td>0.770</td>\n",
       "      <td>0.459</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.453</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.963</td>\n",
       "      <td>133.351</td>\n",
       "      <td>43</td>\n",
       "      <td>Alt. Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What's the Matter Here?</td>\n",
       "      <td>10,000 Maniacs</td>\n",
       "      <td>1987</td>\n",
       "      <td>291173</td>\n",
       "      <td>4</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.816</td>\n",
       "      <td>9</td>\n",
       "      <td>-7.293</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.00449</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>0.519</td>\n",
       "      <td>99.978</td>\n",
       "      <td>12</td>\n",
       "      <td>Alt. Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trouble Me</td>\n",
       "      <td>10,000 Maniacs</td>\n",
       "      <td>1989</td>\n",
       "      <td>193560</td>\n",
       "      <td>4</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.385</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.057</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.15400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.494</td>\n",
       "      <td>117.913</td>\n",
       "      <td>47</td>\n",
       "      <td>Alt. Rock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Candy Everybody Wants</td>\n",
       "      <td>10,000 Maniacs</td>\n",
       "      <td>1992</td>\n",
       "      <td>185960</td>\n",
       "      <td>4</td>\n",
       "      <td>0.622</td>\n",
       "      <td>0.876</td>\n",
       "      <td>10</td>\n",
       "      <td>-6.310</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.01930</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>0.867</td>\n",
       "      <td>104.970</td>\n",
       "      <td>43</td>\n",
       "      <td>Alt. Rock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Track          Artist  Year  Duration  Time_Signature  \\\n",
       "0         Hey Jack Kerouac  10,000 Maniacs  1987    206413               4   \n",
       "1         Like the Weather  10,000 Maniacs  1987    236653               4   \n",
       "2  What's the Matter Here?  10,000 Maniacs  1987    291173               4   \n",
       "3               Trouble Me  10,000 Maniacs  1989    193560               4   \n",
       "4    Candy Everybody Wants  10,000 Maniacs  1992    185960               4   \n",
       "\n",
       "   Danceability  Energy  Key  Loudness  Mode  Speechiness  Acousticness  \\\n",
       "0         0.616   0.511    6   -15.894     1       0.0279       0.03840   \n",
       "1         0.770   0.459    1   -17.453     1       0.0416       0.11200   \n",
       "2         0.593   0.816    9    -7.293     1       0.0410       0.00449   \n",
       "3         0.861   0.385    2   -10.057     1       0.0341       0.15400   \n",
       "4         0.622   0.876   10    -6.310     1       0.0305       0.01930   \n",
       "\n",
       "   Instrumentalness  Liveness  Valence    Tempo  Popularity      Genre  \n",
       "0          0.000000    0.1500    0.604  132.015          40  Alt. Rock  \n",
       "1          0.003430    0.1450    0.963  133.351          43  Alt. Rock  \n",
       "2          0.000032    0.0896    0.519   99.978          12  Alt. Rock  \n",
       "3          0.000000    0.1230    0.494  117.913          47  Alt. Rock  \n",
       "4          0.006840    0.0987    0.867  104.970          43  Alt. Rock  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('ClassicHit.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Track                object\n",
       "Artist               object\n",
       "Year                  int64\n",
       "Duration              int64\n",
       "Time_Signature        int64\n",
       "Danceability        float64\n",
       "Energy              float64\n",
       "Key                   int64\n",
       "Loudness            float64\n",
       "Mode                  int64\n",
       "Speechiness         float64\n",
       "Acousticness        float64\n",
       "Instrumentalness    float64\n",
       "Liveness            float64\n",
       "Valence             float64\n",
       "Tempo               float64\n",
       "Popularity            int64\n",
       "Genre                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the columns and their data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Since the Track and Artist are strings, we will be dropping those columns. In theory, we could find a way to encode those, but for now we will be focusing on predicting the genre solely based on the numerical features. We will also remove the Year, Duration and Time Signature columns since those are generic pieces of information about the songs that will likely not yield any new info\n",
    "\n",
    "<h3> Additionally, we will do the same basic preprocessing steps as last week where we drop any duplicate rows and remove rows with a tempo of 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Danceability        float64\n",
       "Energy              float64\n",
       "Key                   int64\n",
       "Loudness            float64\n",
       "Mode                  int64\n",
       "Speechiness         float64\n",
       "Acousticness        float64\n",
       "Instrumentalness    float64\n",
       "Liveness            float64\n",
       "Valence             float64\n",
       "Tempo               float64\n",
       "Popularity            int64\n",
       "Genre                object\n",
       "dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop Track and Artist columns\n",
    "df = df.drop(['Track', 'Artist', 'Year', 'Duration', 'Time_Signature'], axis=1)\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> With some models, when you have a multi-class classification problem like this, you may need to go and utilize one-hot encoding or label encoding to deal with the multiple-class issue. This is especially the case when the labels you have for each class are strings as opposed to numerical. For simplicity, we will use label encoding since it doesn't create additional columns that we need to deal with.\n",
    "\n",
    "<h3> Label encoding works by assigning each class a number as a way to convert the strings into numbers. While this is convenient, it may cause the models to believe that there are inherent relationships between genres that may not actually exist (i.e. if Rock is 0 but Classical is 1, a model may think they're similar when they aren't). This is where one-hot encoding would be better since it avoids this \"inherent bias\" problem, but it does create a new column for every class. Since we have 19 genres, this is a lot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Key</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Instrumentalness</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.616</td>\n",
       "      <td>0.511</td>\n",
       "      <td>6</td>\n",
       "      <td>-15.894</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>0.03840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.604</td>\n",
       "      <td>132.015</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.770</td>\n",
       "      <td>0.459</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.453</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.963</td>\n",
       "      <td>133.351</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.593</td>\n",
       "      <td>0.816</td>\n",
       "      <td>9</td>\n",
       "      <td>-7.293</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.00449</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>0.519</td>\n",
       "      <td>99.978</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.861</td>\n",
       "      <td>0.385</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.057</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.15400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.494</td>\n",
       "      <td>117.913</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.622</td>\n",
       "      <td>0.876</td>\n",
       "      <td>10</td>\n",
       "      <td>-6.310</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.01930</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>0.867</td>\n",
       "      <td>104.970</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Danceability  Energy  Key  Loudness  Mode  Speechiness  Acousticness  \\\n",
       "0         0.616   0.511    6   -15.894     1       0.0279       0.03840   \n",
       "1         0.770   0.459    1   -17.453     1       0.0416       0.11200   \n",
       "2         0.593   0.816    9    -7.293     1       0.0410       0.00449   \n",
       "3         0.861   0.385    2   -10.057     1       0.0341       0.15400   \n",
       "4         0.622   0.876   10    -6.310     1       0.0305       0.01930   \n",
       "\n",
       "   Instrumentalness  Liveness  Valence    Tempo  Popularity  Genre  \n",
       "0          0.000000    0.1500    0.604  132.015          40      0  \n",
       "1          0.003430    0.1450    0.963  133.351          43      0  \n",
       "2          0.000032    0.0896    0.519   99.978          12      0  \n",
       "3          0.000000    0.1230    0.494  117.913          47      0  \n",
       "4          0.006840    0.0987    0.867  104.970          43      0  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Label encode the Genre column\n",
    "le = LabelEncoder()\n",
    "df['Genre'] = le.fit_transform(df['Genre'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> We can now split the data into features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "15145    18\n",
       "15146    18\n",
       "15147    18\n",
       "15148    18\n",
       "15149    18\n",
       "Name: Genre, Length: 15150, dtype: int32"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(['Genre'], axis=1)\n",
    "y = df['Genre']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Split data into train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      780\n",
       "1      683\n",
       "2      833\n",
       "3      652\n",
       "4      700\n",
       "5      575\n",
       "6      388\n",
       "7      311\n",
       "8      778\n",
       "9      922\n",
       "10    3669\n",
       "11     754\n",
       "12     822\n",
       "13     718\n",
       "14     439\n",
       "15     799\n",
       "16     381\n",
       "17     620\n",
       "18     326\n",
       "Name: Genre, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the counts of each genre, sorted by genre\n",
    "df['Genre'].value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Notice how there are so many more entries for Genre 10 than all the other genres? If we're not careful, the model might become really good at predicting Genre 10 but none of the others. For this reason, when we do our train_test split we want to <i>stratify</i> our splitting so that way each class is equally represented across the train/test splits\n",
    "\n",
    "<h3> Additionally, we don't want our results to be determined purely by just one random split of the data. What if there are other splits where the models perform better? For this reason, we will use k-fold cross-validation to test the models on multiple splits to get a better overall evaluation of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment below for regular train/test split\n",
    "# 80% training, 20% testing, random_state=42 for reproducibility\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) \n",
    "\n",
    "cv = 5 # Number of cross-validation folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Danceability</th>\n",
       "      <th>Energy</th>\n",
       "      <th>Key</th>\n",
       "      <th>Loudness</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Speechiness</th>\n",
       "      <th>Acousticness</th>\n",
       "      <th>Instrumentalness</th>\n",
       "      <th>Liveness</th>\n",
       "      <th>Valence</th>\n",
       "      <th>Tempo</th>\n",
       "      <th>Popularity</th>\n",
       "      <th>Genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.616</td>\n",
       "      <td>0.511</td>\n",
       "      <td>6</td>\n",
       "      <td>-15.894</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>0.03840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>0.604</td>\n",
       "      <td>132.015</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.770</td>\n",
       "      <td>0.459</td>\n",
       "      <td>1</td>\n",
       "      <td>-17.453</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0416</td>\n",
       "      <td>0.11200</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>0.963</td>\n",
       "      <td>133.351</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.593</td>\n",
       "      <td>0.816</td>\n",
       "      <td>9</td>\n",
       "      <td>-7.293</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>0.00449</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.0896</td>\n",
       "      <td>0.519</td>\n",
       "      <td>99.978</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.861</td>\n",
       "      <td>0.385</td>\n",
       "      <td>2</td>\n",
       "      <td>-10.057</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0341</td>\n",
       "      <td>0.15400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.1230</td>\n",
       "      <td>0.494</td>\n",
       "      <td>117.913</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.622</td>\n",
       "      <td>0.876</td>\n",
       "      <td>10</td>\n",
       "      <td>-6.310</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0305</td>\n",
       "      <td>0.01930</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>0.867</td>\n",
       "      <td>104.970</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Danceability  Energy  Key  Loudness  Mode  Speechiness  Acousticness  \\\n",
       "0         0.616   0.511    6   -15.894     1       0.0279       0.03840   \n",
       "1         0.770   0.459    1   -17.453     1       0.0416       0.11200   \n",
       "2         0.593   0.816    9    -7.293     1       0.0410       0.00449   \n",
       "3         0.861   0.385    2   -10.057     1       0.0341       0.15400   \n",
       "4         0.622   0.876   10    -6.310     1       0.0305       0.01930   \n",
       "\n",
       "   Instrumentalness  Liveness  Valence    Tempo  Popularity  Genre  \n",
       "0          0.000000    0.1500    0.604  132.015          40      0  \n",
       "1          0.003430    0.1450    0.963  133.351          43      0  \n",
       "2          0.000032    0.0896    0.519   99.978          12      0  \n",
       "3          0.000000    0.1230    0.494  117.913          47      0  \n",
       "4          0.006840    0.0987    0.867  104.970          43      0  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Now it is time to scale. Most columns already look scaled except for Key, Loudness, Mode, Tempo, and Popularity. From these, we want to avoid scaling the Key and Mode since they are just indicators of what key the song was in, and whether it was a major or minor key. \n",
    "\n",
    "<h3> For this reason, we only want to scale the Loudness, Tempo and Popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale just the Loudness, Tempo and Popularity columns\n",
    "columns_to_scale = ['Loudness', 'Tempo', 'Popularity']\n",
    "columns_to_exclude = [col for col in X.columns if col not in columns_to_scale]\n",
    "\n",
    "# Create the ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), columns_to_scale)\n",
    "    ],\n",
    "    remainder='passthrough'  # This will keep the excluded columns as they are\n",
    ")\n",
    "\n",
    "# Fit and transform the data (not running this code for the pipeline later!)\n",
    "# X_train = preprocessor.fit_transform(X_train)\n",
    "# X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Notice how that was a lot of code that we had to write? We didn't even get to the part of writing the code for training the model and getting results! What if there was a nice way we could contain it all together? That's where sklearn's pipelines come in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for getting the model results without needing to repeat code\n",
    "def get_results(model, X_test, y_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    modelAcc = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy:', modelAcc)\n",
    "    print('Precision Recall Fscore Support:', precision_recall_fscore_support(y_test, y_pred, average='weighted'))\n",
    "    return modelAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the hyperSearch function\n",
    "def hyperSearch(pipeline, param_grid, X, y, cv=5):\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=cv, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "    print(f'Best Parameters: {grid_search.best_params_}')\n",
    "    print(f'Best Score: {grid_search.best_score_}')\n",
    "    return grid_search.best_estimator_, grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: all hyperparameters set for models were set after I did the hyperparameter tuning via GridSearch on them. DO NOT RUN THIS UNLESS YOU LIKE YOUR LAPTOP'S CPU TEMPS TO GET REALLY, REALLY HOT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Decision Trees\n",
    "\n",
    "<h3> Decision Trees are a simple model that at the simplest level is like a series of if statements to determine what the result is\n",
    "\n",
    "![Decision Tree](Decision-Trees.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe to store the results from each model\n",
    "performanceSummaryDf = pd.DataFrame(columns=['Model', 'Accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [0.35280528 0.34422442 0.35775578 0.33663366 0.34422442]\n",
      "Mean Accuracy: 0.3471287128712871\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline for the Decision Tree Classifier\n",
    "pipe_dt = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('dt', DecisionTreeClassifier(random_state=42, max_depth=10, min_samples_split=10))\n",
    "])\n",
    "\n",
    "# # Do a grid search for the Decision Tree Classifier\n",
    "# param_grid_dt = {\n",
    "#     'dt__max_depth': [None, 10, 20, 30],\n",
    "#     'dt__min_samples_split': [2, 5, 10]\n",
    "# }\n",
    "# best_dt, best_dt_score = hyperSearch(pipe_dt, param_grid_dt, X, y, cv=cv)\n",
    "\n",
    "dt_scores = cross_val_score(pipe_dt, X, y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores:', dt_scores)\n",
    "print('Mean Accuracy:', dt_scores.mean())\n",
    "# Add the average accuracy to the performance summary dataframe\n",
    "performanceSummaryDf = pd.concat([performanceSummaryDf, pd.DataFrame({'Model': ['Decision Tree'], 'Accuracy': [dt_scores.mean()]})])\n",
    "# Use the below code if you want to use regular train/test split instead of cross-validation\n",
    "# # Fit the pipeline\n",
    "# pipe_dt.fit(X_train, y_train)\n",
    "\n",
    "# # Get the results\n",
    "# dtAcc = get_results(pipe_dt, X_test, y_test)\n",
    "# performanceSummaryDf = pd.concat([performanceSummaryDf, pd.DataFrame({'Model': ['Decision Tree'], 'Accuracy': [dtAcc]})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Random Forests\n",
    "\n",
    "<h3> We saw a Decision Tree on its own doesn't do too great. What if we took a bunch of different decision trees and combined them together to try to get a better result? That's what a Random Forest does.\n",
    "\n",
    "\n",
    "![Random Forest](RF.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [0.42541254 0.42409241 0.42805281 0.39339934 0.40759076]\n",
      "Mean Accuracy: 0.41570957095709565\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline for the Random Forest Classifier\n",
    "pipe_rf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('rf', RandomForestClassifier(random_state=42, max_depth=30, min_samples_split=10, n_estimators=300))\n",
    "])\n",
    "\"\"\"\n",
    "# Do a grid search for the Random Forest Classifier\n",
    "param_grid_rf = {\n",
    "    'rf__n_estimators': [100, 200, 300],\n",
    "    'rf__max_depth': [None, 10, 20, 30],\n",
    "    'rf__min_samples_split': [2, 5, 10]\n",
    "}\n",
    "best_rf, best_rf_score = hyperSearch(pipe_rf, param_grid_rf, X, y, cv=cv)\"\"\"\n",
    "rf_scores = cross_val_score(pipe_rf, X, y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores:', rf_scores)\n",
    "print('Mean Accuracy:', rf_scores.mean())\n",
    "# Add the average accuracy to the performance summary dataframe\n",
    "performanceSummaryDf = pd.concat([performanceSummaryDf, pd.DataFrame({'Model': ['Random Forest'], 'Accuracy': [rf_scores.mean()]})])\n",
    "\n",
    "# Use the below code if you want to use regular train/test split instead of cross-validation\n",
    "# # Fit the pipeline\n",
    "# pipe_rf.fit(X_train, y_train)\n",
    "\n",
    "# # Get the results\n",
    "# rfAcc = get_results(pipe_rf, X_test, y_test)\n",
    "# performanceSummaryDf = pd.concat([performanceSummaryDf, pd.DataFrame({'Model': ['Random Forest'], 'Accuracy': [rfAcc]})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Notice how the results improved when using a whole forest of trees instead of just one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> XGBoost (Extreme Gradient Boosting)\n",
    "\n",
    "<h3> XGBoost is similar to random forests, but XGBoost builds multiple decision trees sequentially, with each tree correcting the errors of its predecessor, generally leading to high predictive accuracy and efficiency.\n",
    "\n",
    "![XGBoost](XGBoost.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [0.42541254 0.42145215 0.4349835  0.3960396  0.40990099]\n",
      "Mean Accuracy: 0.4175577557755775\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline for the XGBoost Classifier\n",
    "pipe_xgb = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('xgb', XGBClassifier(random_state=42, learning_rate = 0.1, max_depth = 3, n_estimators = 200))\n",
    "])\n",
    "\n",
    "# Do a grid search for the XGBoost Classifier\n",
    "\"\"\"param_grid_xgb = {\n",
    "    'xgb__n_estimators': [100, 200, 300],\n",
    "    'xgb__max_depth': [3, 5, 7],\n",
    "    'xgb__learning_rate': [0.1, 0.01, 0.001]\n",
    "}\n",
    "best_xgb, best_xgb_score = hyperSearch(pipe_xgb, param_grid_xgb, X, y, cv=cv)\"\"\"\n",
    "xgb_scores = cross_val_score(pipe_xgb, X, y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores:', xgb_scores)\n",
    "print('Mean Accuracy:', xgb_scores.mean())\n",
    "# Add the average accuracy to the performance summary dataframe\n",
    "performanceSummaryDf = pd.concat([performanceSummaryDf, pd.DataFrame({'Model': ['XGBoost'], 'Accuracy': [xgb_scores.mean()]})])\n",
    "# # Use the below code if you want to use regular train/test split instead of cross-validation\n",
    "# Fit the pipeline\n",
    "# pipe_xgb.fit(X_train, y_train)\n",
    "\n",
    "# # Get the results\n",
    "# xgbAcc = get_results(pipe_xgb, X_test, y_test)\n",
    "# performanceSummaryDf = pd.concat([performanceSummaryDf, pd.DataFrame({'Model': ['XGBoost'], 'Accuracy': [xgbAcc]})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Support Vector Machines (SVMs)\n",
    "\n",
    "<h3> SVMs work by trying to determine a decision boundary between the different classes, where each class falls on one portion of the boundary. These boundaries can be determined by different kernel functions (see below image).\n",
    "\n",
    "![SVC](SVC.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [0.37689769 0.37392739 0.38976898 0.36963696 0.36468647]\n",
      "Mean Accuracy: 0.374983498349835\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline for the Support Vector Classifier\n",
    "pipe_svc = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('svc', SVC(random_state=42, C=10, kernel='rbf'))\n",
    "])\n",
    "\n",
    "# Do a grid search for the Support Vector Classifier\n",
    "\"\"\"param_grid_svc = {\n",
    "    'svc__C': [0.1, 1, 10],\n",
    "    'svc__kernel': ['poly', 'rbf', 'sigmoid']\n",
    "}\n",
    "best_svc, best_svc_score = hyperSearch(pipe_svc, param_grid_svc, X, y, cv=cv)\"\"\"\n",
    "svc_scores = cross_val_score(pipe_svc, X, y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores:', svc_scores)\n",
    "print('Mean Accuracy:', svc_scores.mean())\n",
    "# Add the average accuracy to the performance summary dataframe\n",
    "performanceSummaryDf = pd.concat([performanceSummaryDf, pd.DataFrame({'Model': ['SVC'], 'Accuracy': [svc_scores.mean()]})])\n",
    "# Use the below code if you want to use regular train/test split instead of cross-validation\n",
    "# # Fit the pipeline\n",
    "# pipe_svc.fit(X_train, y_train)\n",
    "\n",
    "# # Get the results\n",
    "# svcAcc = get_results(pipe_svc, X_test, y_test)\n",
    "# performanceSummaryDf = pd.concat([performanceSummaryDf, pd.DataFrame({'Model': ['SVC'], 'Accuracy': [svcAcc]})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> K-Nearest Neighbors (KNN)</h2>\n",
    "\n",
    "<h3>KNN is a instance-based machine learning algorithm that classifies data points based on the majority class among their k nearest neighbors in the feature space. It works by calculating the distances between points, often using metrics like Euclidean distance. Because of all these distance calculations it can be computationally intensive for large datasets.</h3>\n",
    "\n",
    "![KNN](KNN.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Scores: [0.27755776 0.27458746 0.28481848 0.26666667 0.27161716]\n",
      "Mean Accuracy: 0.275049504950495\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline for the K-Nearest Neighbors Classifier\n",
    "pipe_knn = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors=7, weights='distance'))\n",
    "])\n",
    "\n",
    "# Do a grid search for the K-Nearest Neighbors Classifier\n",
    "\"\"\"param_grid_knn = {\n",
    "    'knn__n_neighbors': [3, 5, 7],\n",
    "    'knn__weights': ['uniform', 'distance']\n",
    "}\n",
    "best_knn, best_knn_score = hyperSearch(pipe_knn, param_grid_knn, X, y, cv=cv)\"\"\"\n",
    "knn_scores = cross_val_score(pipe_knn, X, y, cv=cv, scoring='accuracy')\n",
    "print('CV Scores:', knn_scores)\n",
    "print('Mean Accuracy:', knn_scores.mean())\n",
    "# Add the average accuracy to the performance summary dataframe\n",
    "performanceSummaryDf = pd.concat([performanceSummaryDf, pd.DataFrame({'Model': ['K-Nearest Neighbors'], 'Accuracy': [knn_scores.mean()]})])\n",
    "# Use the below code if you want to use regular train/test split instead of cross-validation\n",
    "# # Fit the pipeline\n",
    "# pipe_knn.fit(X_train, y_train)\n",
    "\n",
    "# # Get the results\n",
    "# knnAcc = get_results(pipe_knn, X_test, y_test)\n",
    "# performanceSummaryDf = pd.concat([performanceSummaryDf, pd.DataFrame({'Model': ['K-Nearest Neighbors'], 'Accuracy': [knnAcc]})])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.417558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.415710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVC</td>\n",
       "      <td>0.374983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Average</td>\n",
       "      <td>0.366086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.347129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K-Nearest Neighbors</td>\n",
       "      <td>0.275050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy\n",
       "0              XGBoost  0.417558\n",
       "0        Random Forest  0.415710\n",
       "0                  SVC  0.374983\n",
       "0              Average  0.366086\n",
       "0        Decision Tree  0.347129\n",
       "0  K-Nearest Neighbors  0.275050"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a row for average accuracy \n",
    "performanceSummaryDf = pd.concat([performanceSummaryDf, pd.DataFrame({'Model': ['Average'], 'Accuracy': [performanceSummaryDf['Accuracy'].mean()]})])\n",
    "# Sort the dataframe by accuracy\n",
    "performanceSummaryDf = performanceSummaryDf.sort_values(by='Accuracy', ascending=False)\n",
    "performanceSummaryDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Does anyone have any guesses as to why all of the models performed as they did? </h1>\n",
    "\n",
    "Too many classes? Features between groups having too similar of value to be distinguishable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Next Steps\n",
    "\n",
    "<h3> There are many other things that we can do to improve our model performance. Some options include: </h3>\n",
    "\n",
    "- <h4> Do more extensive hyperparameter tuning in the GridSearch (takes a lot of time!)\n",
    "- <h4> Adding/removing features to see which ones help/worsen the models\n",
    "- <h4> Simplify the problem by reducing the number of classes we're predicting\n",
    "- <h4> and more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
